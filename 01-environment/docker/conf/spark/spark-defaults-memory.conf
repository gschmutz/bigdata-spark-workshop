
# ======== Event Log ========

# event log and history is stored on a local folder "mapped" to the docker host so that it is available for Spark History service (no HDFS or S3 necessary)
spark.history.provider         org.apache.spark.deploy.history.FsHistoryProvider
spark.eventLog.dir             file:///var/log/spark/logs
spark.history.fs.logDirectory  file:///var/log/spark/logs

spark.eventLog.enabled true
spark.eventLog.overwrite true

# ======== S3 Access ========
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint http://minio:9000
spark.hadoop.fs.s3a.path.style.access true

# specify AWS dependencies to access S3 as packages which are loaded via Maven (should no longer be necessary)
# spark.jars.packages   com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3
spark.jars.packages 	io.delta:delta-core_2.11:0.5.0

#spark.driver.extraClassPath 	/hadoop-2.7.3/share/hadoop/tools/lib/*
#spark.executor.extraClassPath	/hadoop-2.7.3/share/hadoop/tools/lib/*

# Necessary for Frankfurt to work with S3
spark.driver.extraJavaOptions    -Dcom.amazonaws.services.s3.enableV4
spark.executor.extraJavaOptions  -Dcom.amazonaws.services.s3.enableV4

# ======== Hive Metastore ========
spark.sql.catalogImplementation=in-memory

# ======== YARN  ========

#  Comma-separated list of files to be placed in the working directory of each executor.
spark.yarn.dist.files            /etc/spark/conf/hive-site.xml


# ======== Not Used ========

#spark.yarn.historyServer.address=analyticsplatform:18080
