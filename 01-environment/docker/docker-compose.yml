# =======================================================================
# Platform Name            bigdata-minio-platform
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  develop
# =======================================================================
networks:
  default:
    name: bigdata-minio-platform
services:
  #  ================================== Apache Spark 2.x ========================================== #
  spark-master:
    image: bitnami/spark:3.5.3
    container_name: spark-master
    hostname: spark-master
    labels:
      com.platys.name: spark
      com.platys.description: Spark Master Node
      com.platys.webui.title: Spark UI
      com.platys.webui.url: http://dataplatform:28304
    ports:
      - 28304:28304
      - 6066:6066
      - 7077:7077
      - 4040-4044:4040-4044
    environment:
      # bitnami env vars
      SPARK_MODE: master
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_MASTER_WEBUI_PORT: 28304
      SPARK_MASTER_OPTS: -Dspark.deploy.defaultCores=2
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
#     INIT_DAEMON_STEP: setup_spark
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_cores_max: 2
      SPARK_DEFAULTS_CONF_spark_executor_memory: 2048mb
#      SPARK_DEFAULTS_CONF_spark_sql_extensions: __omit_place_holder__3947ffad85dda43fff38a7fcc7f68ff37d0e20ea
#      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: __omit_place_holder__3947ffad85dda43fff38a7fcc7f68ff37d0e20ea
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.postgresql:postgresql:42.3.4,org.apache.spark:spark-avro_2.12:3.5.2,graphframes:graphframes:0.8.4-spark3.5-s_2.12
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-spark_2.12:3.2.1,io.delta:delta-storage:3.2.1,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars: /opt/bitnami/spark/jars/delta-spark_2.12-3.2.1.jar,/opt/bitnami/spark/jars/delta-storage-3.2.1.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./scripts/spark/pyspark:/opt/bitnami/spark/bin/pyspark
      - ./container-volume/spark/logs/:/var/log/spark/logs
#      - ./scripts/docker/maven-download.sh:/usr/src/app/maven-download.sh
      - spark-3-5-3-vol:/opt/bitnami/spark
    restart: unless-stopped
    healthcheck:
      test: [CMD, spark-submit, --version]
      interval: 30s
      timeout: 10s
      retries: 5
  spark-worker-1:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-1
    hostname: spark-worker-1
    labels:
      com.platys.name: spark
      com.platys.description: Spark Worker Node
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - 28111:28111
    environment:
      # bitnami env vars
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_WORKER_WEBUI_PORT: '28111'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_cores_max: 2
      SPARK_DEFAULTS_CONF_spark_executor_memory: 2048mb
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.postgresql:postgresql:42.3.4,org.apache.spark:spark-avro_2.12:3.5.2,graphframes:graphframes:0.8.4-spark3.5-s_2.12,
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-spark_2.12:3.2.1,io.delta:delta-storage:3.2.1,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars: /opt/bitnami/spark/jars/delta-spark_2.12-3.2.1.jar,/opt/bitnami/spark/jars/delta-storage-3.2.1.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-2:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-2
    hostname: spark-worker-2
    labels:
      com.platys.name: spark
      com.platys.description: Spark Worker Node
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - 28112:28112
    environment:
      # bitnami env vars
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_WORKER_WEBUI_PORT: '28112'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_cores_max: 2
      SPARK_DEFAULTS_CONF_spark_executor_memory: 2048mb
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.postgresql:postgresql:42.3.4,org.apache.spark:spark-avro_2.12:3.5.2,graphframes:graphframes:0.8.4-spark3.5-s_2.12,
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-spark_2.12:3.2.1,io.delta:delta-storage:3.2.1,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars: /opt/bitnami/spark/jars/delta-spark_2.12-3.2.1.jar,/opt/bitnami/spark/jars/delta-storage-3.2.1.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-history:
    image: bitnami/spark:3.5.3
    container_name: spark-history
    hostname: spark-history
    labels:
      com.platys.name: spark-historyserver
      com.platys.description: Spark History Server
      com.platys.webui.title: Spark History Server
      com.platys.webui.url: http://dataplatform:28117
      com.platys.restapi.title: Spark History Server
      com.platys.restapi.url: http://dataplatform:28117/api/v1
    expose:
      - 18080
    ports:
      - 28117:18080
    environment:
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.postgresql:postgresql:42.3.4,org.apache.spark:spark-avro_2.12:3.5.2,graphframes:graphframes:0.8.4-spark3.5-s_2.12,
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-spark_2.12:3.2.1,io.delta:delta-storage:3.2.1,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars: /opt/bitnami/spark/jars/delta-spark_2.12-3.2.1.jar,/opt/bitnami/spark/jars/delta-storage-3.2.1.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /opt/bitnami/spark/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_cores_max: 2
      SPARK_DEFAULTS_CONF_spark_executor_memory: 2048mb
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    restart: unless-stopped
  spark-thriftserver:
    image: bitnami/spark:3.5.3
    container_name: spark-thriftserver
    hostname: spark-thriftserver
    labels:
      com.platys.name: spark-thriftserver
      com.platys.description: Spark Thriftserver
      com.platys.webui.title: Spark Thriftserver UI
      com.platys.webui.url: http://dataplatform:28298
    ports:
      - 28118:10000
      - 28298:4040
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.postgresql:postgresql:42.3.4,org.apache.spark:spark-avro_2.12:3.5.2,graphframes:graphframes:0.8.4-spark3.5-s_2.12,
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-spark_2.12:3.2.1,io.delta:delta-storage:3.2.1,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars: /opt/bitnami/spark/jars/delta-spark_2.12-3.2.1.jar,/opt/bitnami/spark/jars/delta-storage-3.2.1.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      # we override the run.sh script to start thriftserver instead of master/worker
      - ./scripts/spark/run-thriftserver.sh:/opt/bitnami/scripts/spark/run.sh
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    restart: unless-stopped
  #  ================================== Apache Hive Metastore ========================================== #
  hive-metastore:
    image: trivadis/apache-hive:3.1.2-postgresql-metastore-s3
    container_name: hive-metastore
    hostname: hive-metastore
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore
    ports:
      - 9083:9083
    environment:
      CORE_CONF_fs_defaultFS: file:///tmp
      HIVE_SITE_CONF_fs_defaultFS: file:///tmp
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-metastore:9083
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://hive-metastore-db/metastore
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: hive
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: false
      HIVE_SITE_CONF_hive_metastore_event_db_notification_api_auth: false
      # necessary for Trino to be able to read from Avro
      HIVE_SITE_CONF_metastore_storage_schema_reader_impl: org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader
      SERVICE_PRECONDITION: hive-metastore-db:5432
    volumes:
      - ./data-transfer:/data-transfer
    command: /opt/hive/bin/hive --service metastore
    restart: unless-stopped
  hive-metastore-db:
    image: trivadis/apache-hive-metastore-postgresql:3.1.0-postgres9.5.3
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore DB
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Apache Avro Tools ========================================== #
  avro-tools:
    image: trivadis/apache-avro-tools:latest
    hostname: avro-tools
    container_name: avro-tools
    labels:
      com.platys.name: avro-tools
      com.platys.description: Avro Tools
    volumes:
      - ./data-transfer:/data-transfer
#    entrypoint: "tail -f /dev/null"
    tty: true
#    restart: unless-stopped
  #  ================================== Apache Parquet Tools ========================================== #
  parquet-tools:
    image: nathanhowell/parquet-tools:latest
    hostname: parquet-tools
    container_name: parquet-tools
    labels:
      com.platys.name: parquet-tools
      com.platys.description: Parquet Tools
    volumes:
      - ./data-transfer:/data-transfer
#    entrypoint: "tail -f /dev/null"
    tty: true
#    restart: unless-stopped
  #  ================================== NiFi 2 ========================================== #
  nifi2-1:
    image: apache/nifi:2.2.0
    container_name: nifi2-1
    hostname: nifi2-1
    labels:
      com.platys.name: nifi
      com.platys.description: NiFi Data Integration Engine (V2)
      com.platys.webui.title: Apache NiFi UI
      com.platys.webui.url: https://dataplatform:18083/nifi
      com.platys.restapi.title: Apche NiFi REST API
      com.platys.restapi.url: https://dataplatform:18083/nifi-api
      com.platys.password.envvars: PLATYS_NIFI2_PASSWORD
    ports:
      # HTTP
      - 18083:18083
      # Remote Input Socket
      - 10015:10015/tcp
      # Prometheus Port
      - 1273:1234
    environment:
      NIFI_WEB_HTTPS_PORT: '18083'
      NIFI_WEB_HTTPS_HOST: 0.0.0.0
      NIFI_WEB_PROXY_HOST: ${PUBLIC_IP}:18083,${DOCKER_HOST_IP}:18083
      NIFI_SECURITY_USER_AUTHORIZER: single-user-authorizer
      NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER: single-user-provider
      S3_ENDPOINT: http://minio-1:9000
      S3_PATH_STYLE_ACCESS: 'true'
      S3_REGION: us-east-1
      # these two env variables are also needed for the s3-credentials.properties file gen to work! 
      AWS_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      NIFI_REMOTE_INPUT_SOCKET_PORT: '10015'
      NIFI_REMOTE_INPUT_HOST: nifi-1
      NIFI_SENSITIVE_PROPS_KEY: 12345678901234567890A
      SINGLE_USER_CREDENTIALS_USERNAME: nifi
      SINGLE_USER_CREDENTIALS_PASSWORD: ${PLATYS_NIFI2_PASSWORD:-1234567890ACD}
      INITIAL_ADMIN_IDENTITY: nifi
      # these config settings are custom ones and treated by the pre-start.sh script before the standard start.sh script of the NiFi docker image
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/nifi2/pre-start.sh:/tmp/pre-start.sh
      - ./plugins/nifi2/nar_extensions/:/opt/nifi/nifi-current/nar_extensions/
      - ./plugins/nifi2/python_extensions/:/opt/nifi/nifi-current/python_extensions/
      - aws-credentials-vol:/opt/nifi/nifi-current/.aws:RO
      - nifi-config-vol:/opt/nifi/nifi-current/conf
    entrypoint:
      # We override the entrypoint from the docker image and therefore have to run ../scripts/start.sh from the NiFi docker image
      - bash
      - -c
      - |
        /tmp/pre-start.sh        
        ../scripts/start.sh
    restart: unless-stopped
#  ================================== Airflow ========================================== #
  airflow:
    image: apache/airflow:2.10.2-python3.10
    container_name: airflow
    hostname: airflow
    labels:
      com.platys.name: airflow
      com.platys.description: Job Orchestration & Scheduler
      com.platys.webui.title: Airflow UI
      com.platys.webui.url: http://dataplatform:28139
      com.platys.restapi.title: Airflow REST API (dags as sample)
      com.platys.restapi.url: http://dataplatform:28139/api/v1/dags
      com.platys.password.envvars: PLATYS_AIRFLOW_SECRET
    ports:
      - 28139:8080
    command: webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= ''
      - AIRFLOW__WEBSERVER__SECRET_KEY=${PLATYS_AIRFLOW_SECRET:-abc123!}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=True
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.session
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-apache-spark
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=60
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - SPARK_HOME=/opt/spark
      - AIRFLOW_VAR_S3_ENDPOINT=http://minio-1:9000
      - AIRFLOW_VAR_S3_ACCESS_KEY=${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      - AIRFLOW_VAR_S3_SECRET_ACCESS_KEY=${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      - AIRFLOW_VAR_S3_PATH_STYLE_ACCESS='true'
      - AIRFLOW__OPENLINEAGE__DISABLED=true
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/airflow/dags:/opt/airflow/dags:Z
      - ./plugins/airflow/:/opt/airflow/plugins:Z
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - spark-3-5-3-vol:/opt/spark:RO
      - aws-credentials-vol:/home/airflow/.aws:RO
    user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, --fail, http://localhost:8080/health]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
  airflow-scheduler:
    image: apache/airflow:2.10.2-python3.10
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    labels:
      com.platys.name: airflow
      com.platys.description: Job Orchestration & Scheduler
    command:
      - bash
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 airflow-db:5432
        airflow scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= ''
      - AIRFLOW__WEBSERVER__SECRET_KEY=abc123!
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=True
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.session
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-apache-spark
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=60
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - SPARK_HOME=/opt/spark
      - AIRFLOW_VAR_S3_ENDPOINT=http://minio-1:9000
      - AIRFLOW_VAR_S3_ACCESS_KEY=${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      - AIRFLOW_VAR_S3_SECRET_ACCESS_KEY=${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      - AIRFLOW_VAR_S3_PATH_STYLE_ACCESS='true'
      - AIRFLOW__OPENLINEAGE__DISABLED=true
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/airflow/dags:/opt/airflow/dags:Z
      - ./plugins/airflow/:/opt/airflow/plugins:Z
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - spark-3-5-3-vol:/opt/spark:RO
      - aws-credentials-vol:/home/airflow/.aws:RO
    user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, --fail, http://localhost:8974/health]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
  airflow-db:
    image: postgres:13
    container_name: airflow-db
    hostname: airflow-db
    labels:
      com.platys.name: airflow
      com.platys.description: Job Orchestration & Scheduler
      com.platys.password.envvars: PLATYS_AIRFLOW_POSTGRESQL_PASSWORD
    environment:
      - POSTGRES_DB=airflowdb
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  airflow-init:
    image: apache/airflow:2.10.2-python3.10
    container_name: airflow-init
    hostname: airflow-init
    labels:
      com.platys.password.envvars: PLATYS_AIRFLOW_ADMIN_PASSWORD,PLATYS_AIRFLOW_SECRET_KEY
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        function ver() {
          printf "%04d%04d%04d%04d" $${1//./ }
        }
        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && airflow version)
        airflow_version_comparable=$$(ver $${airflow_version})
        min_airflow_version=2.2.0
        min_airflow_version_comparable=$$(ver $${min_airflow_version})
        if (( airflow_version_comparable < min_airflow_version_comparable )); then
          echo
          echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
          echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
          echo
          exit 1
        fi
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= ''
      - AIRFLOW__WEBSERVER__SECRET_KEY=${PLATYS_AIRFLOW_SECRET_KEY:-abc123!}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=True
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.session
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-apache-spark
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=60
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - _AIRFLOW_DB_MIGRATE=True
      - _AIRFLOW_WWW_USER_CREATE=True
      - _AIRFLOW_WWW_USER_USERNAME=airflow
      - _AIRFLOW_WWW_USER_PASSWORD=${PLATYS_AIRFLOW_ADMIN_PASSWORD:-abc123!}
    volumes:
      - ./data-transfer:/data-transfer
    user: 0:0
    init: true
  #  ================================== Zeppelin ========================================== #
  zeppelin:
    image: apache/zeppelin:0.11.2
    container_name: zeppelin
    hostname: zeppelin
    labels:
      com.platys.name: zeppelin
      com.platys.description: Data Science Notebook
      com.platys.webui.title: Apache Zeppelin UI
      com.platys.webui.url: http://dataplatform:28080
      com.platys.password.envvars: PLATYS_ZEPPELIN_ADMIN_PASSWORD,PLATYS_ZEPPELIN_USER_PASSWORD,PLATYS_AWS_SECRET_ACCESS_KEY
    ports:
      - 28080:8080
      - 6060:6060
      - 5050:5050
      - 4050-4054:4050-4054
    environment:
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_DEFAULT_REGION: us-east-1
      SPARK_HOME: /opt/bitnami/spark
      ZEPPELIN_SPARK_MASTER: spark://spark-master:7077
      SPARK_MASTER: spark://spark-master:7077
      SPARK_SUBMIT_OPTIONS: ' --conf spark.ui.port=4050 --conf spark.driver.host=zeppelin --conf spark.driver.port=5050 --conf spark.driver.bindAddress=0.0.0.0 --conf spark.blockManager.port=6060 --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4'
      PYSPARK_PYTHON: python3
      PYSPARK_DRIVER_PYTHON: python3
      # Zeppelin Properties
      ZEPPELIN_ADDR: 0.0.0.0
      ZEPPELIN_PORT: '8080'
      ZEPPELIN_MEM: -Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=512m
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      ZEPPELIN_INTERPRETER_DEP_MVNREPO: https://repo.maven.apache.org/maven2
      ZEPPELIN_NOTEBOOK_DIR: notebook
      ZEPPELIN_NOTEBOOK_CRON_ENABLE: 'True'
      # Will replace values in shiro.ini
      ZEPPELIN_ADMIN_USERNAME: admin
      ZEPPELIN_ADMIN_PASSWORD: ${PLATYS_ZEPPELIN_ADMIN_PASSWORD:-changeme}
      ZEPPELIN_USER_USERNAME: zeppelin
      ZEPPELIN_USER_PASSWORD: ${PLATYS_ZEPPELIN_USER_PASSWORD:-changeme}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/zeppelin:/opt/zeppelin/conf.templates
      - ./init/zeppelin/zeppelin-init.sh:/opt/zeppelin/bin/zeppelin-init.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./conf/awscli/s3cfg.template:/root/.s3cfg.template
      - spark-3-5-3-vol:/opt/bitnami/spark:RO
    command:
      - bash
      - -c
      - |
        ./bin/zeppelin-init.sh
        ./bin/zeppelin.sh
    restart: unless-stopped
  #  ================================== Jupyter ========================================== #
  jupyter:
    image: quay.io/jupyter/all-spark-notebook:spark-3.5.3
    container_name: jupyter
    hostname: jupyter
    labels:
      com.platys.name: jupyter
      com.platys.description: Web-based interactive development environment for notebooks, code, and data
      com.platys.webui.title: Jupyter UI
      com.platys.webui.url: http://dataplatform:28888
      com.platys.password.envvars: PLATYS_JUPYTER_TOKEN,PLATYS_AWS_SECRET_ACCESS_KEY
    ports:
      - 28888:8888
      - 28376-28380:4040-4044
    user: root
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      JUPYTER_ENABLE_LAB: "'yes'"
      GRANT_SUDO: "'yes'"
      JUPYTER_TOKEN: ${PLATYS_JUPYTER_TOKEN:-abc123!}
      DOCKER_STACKS_JUPYTER_CMD: lab
      MAVEN_DOWNLOAD_JARS: com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.4,com.google.guava:guava:27.1-jre
      # remove some JARS if they are conflicting with the ones installed above
      REMOVE_JARS: guava-14.0.1.jar
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/jupyter/on-startup-jupyter/:/usr/local/bin/start-notebook.d/
      - ./init/jupyter/on-startup-jupyter-finished/:/usr/local/bin/before-notebook.d/
      - ./init/jupyter/on-startup-notebook-kernel:/home/jovyan/.ipython/profile_default/startup/
      - ./scripts/docker/maven-download.sh:/maven-download.sh
    #  - "./conf/jupyter/spark-defaults.conf:/usr/local/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf"
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        start-notebook.sh
    restart: unless-stopped
  #  ================================== PostgreSQL ========================================== #
  postgresql:
    image: postgres:17
    container_name: postgresql
    hostname: postgresql
    labels:
      com.platys.name: postgresql
      com.platys.description: Open-Source object-relational database system
      com.platys.password.envvars: PLATYS_POSTGRESQL_PASSWORD,PLATYS_POSTGRESQL_MULTIPLE_PASSWORD
    ports:
      - 5432:5432
    environment:
      - POSTGRES_PASSWORD=${PLATYS_POSTGRESQL_PASSWORD:-abc123!}
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
      - POSTGRES_MULTIPLE_DATABASES=demodb
      - POSTGRES_MULTIPLE_USERS=demo
      - POSTGRES_MULTIPLE_PASSWORDS=${PLATYS_POSTGRESQL_MULTIPLE_PASSWORD:-abc123!}
      - POSTGRES_MULTIPLE_ADDL_ROLES=
      - PGDATA=/var/lib/postgresql/data/pgdata
      - DB_SCHEMA=demo
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    restart: unless-stopped
    healthcheck:
      test: [CMD-SHELL, pg_isready -U postgres]
      interval: 10s
      timeout: 5s
      retries: 5
  #  ================================== Trino ========================================== #
  trino-1:
    image: trinodb/trino:475
    hostname: trino-1
    container_name: trino-1
    labels:
      com.platys.name: trino
      com.platys.description: SQL Virtualization Engine
      com.platys.webui.title: Trino UI
      com.platys.webui.url: http://dataplatform:28082/ui/preview
    ports:
      - 28082:8080
      - 28083:8443
    environment:
      # this is only generated to keep the structure valid if no other env variables are present
      DUMMY: make-it-valid
      S3_ENDPOINT: http://minio-1:9000
      S3_REGION: us-east-1
      S3_AWS_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      S3_AWS_SECRET_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      S3_PATH_STYLE_ACCESS: 'true'
      HIVE_STORAGE_FORMAT: ORC
      HIVE_COMPRESSION_CODEC: GZIP
      HIVE_VIEWS_ENABLED: 'false'
      HIVE_RUN_AS_INVOKER: 'false'
      HIVE_LEGACY_TRANSLATION: 'false'
      NESSIE_CATALOG_WAREHOUSE_DIR: s3a://admin-bucket/hive/warehouse
      POSTGRESQL_DATABASE: postgres
      POSTGRESQL_USER: demo
      POSTGRESQL_PASSWORD: abc123!
      EVENT_LISTENER_CONFIG_FILES: ''
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/trino/single/config.properties:/etc/trino/config.properties
      - ./conf/trino/single/node.properties:/etc/trino/node.properties
      - ./conf/trino/catalog/minio.properties:/etc/trino/catalog/minio.properties
      - ./conf/trino/catalog/delta.properties:/etc/trino/catalog/delta.properties
      - ./conf/trino/catalog/postgresql.properties:/etc/trino/catalog/postgresql.properties
      - ./custom-conf/trino/security:/etc/trino/security
    restart: unless-stopped
  #  ================================== Minio ========================================== #
  minio-1:
    image: minio/minio:RELEASE.2025-02-07T23-21-09Z
    container_name: minio-1
    hostname: minio-1
    labels:
      com.platys.name: minio
      com.platys.description: Software-defined Object Storage
      com.platys.webui.title: MinIO UI
      com.platys.webui.url: http://dataplatform:9010
      com.platys.password.envvars: PLATYS_AWS_SECRET_ACCESS_KEY
    ports:
      # S3 API Port
      - 9000:9000
      # UI Port
      - 9010:9010
    environment:
      MINIO_ROOT_USER: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      MINIO_ROOT_PASSWORD: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      # remove region due to problems with RisingWave
      #MINIO_REGION_NAME: us-east-1
      #MINIO_REGION: us-east-1
      MINIO_DOMAIN: minio
      MINIO_SERVER_URL: http://${PUBLIC_IP}:9000
      MINIO_COMPRESSION_ENABLE: off
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus-1:9090
    volumes:
      - ./data-transfer:/data-transfer
    command: server /data --console-address ":9010"
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, -f, http://minio-1:9000/minio/health/live]
      interval: 15s
      timeout: 20s
      retries: 3
  #  ================================== Minio MC ========================================== #
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    hostname: minio-mc
    labels:
      com.platys.name: minio
      com.platys.description: MinIO Console
    environment:
      # these two env variables are also needed for the s3-credentials.properties file gen to work! 
      AWS_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      MC_HOST_minio-1: http://${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}:${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}@minio-1:9000
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - ./security/aws/credentials:/tmp/credentials.templ
      - aws-credentials-vol:/tmp/.aws:RO
    entrypoint:
      - /bin/sh
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 minio-1:9000
        mkdir -p /tmp/.aws
        eval "echo \"$$(cat /tmp/credentials.templ)\"" >> /tmp/.aws/credentials
        mc mb --ignore-existing minio-1/admin-bucket
              for bucket in $$(tr ',' '\n' <<< "")
        do
          mc mb --ignore-existing minio-1/$$bucket
        done
        #
        while [ 1 -eq 1 ];do sleep 60;done
    restart: unless-stopped
  #  ================================== Awscli ========================================== #
  awscli:
    image: trivadis/awscli-s3cmd:latest
    container_name: awscli
    hostname: awscli
    labels:
      com.platys.name: awscli
      com.platys.description: AWS CLI
    environment:
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:-V42FCGRVMK24JJ8DHUYG}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: minio-1:9000
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/awscli/s3cfg.template:/root/.s3cfg.template
    command: tail -f /dev/null
    restart: unless-stopped
  #  ================================== File Browser ================= #
  filebrowser:
    image: hurlenko/filebrowser:latest
    container_name: filebrowser
    hostname: filebrowser
    labels:
      com.platys.name: filebrowser
      com.platys.description: File-Browser
      com.platys.webui.title: File Browser UI
      com.platys.webui.url: http://dataplatform:28178/filebrowser
      com.platys.password.envvars: PLATYS_FILEBROWSER_PASSWORD_HASH10
    ports:
      - 28178:8080
    environment:
      - FB_BASEURL=/filebrowser
      - FB_USERNAME=admin
    volumes:
      # we map the data-transfer folder to /data so that Filebrowser "sits on top" of the data-transfer folder
      - ./data-transfer:/data
    user: 1000:1000
    restart: unless-stopped
  #  ================================== Wetty ========================================== #
  wetty:
    image: wettyoss/wetty:latest
    container_name: wetty
    hostname: wetty
    labels:
      com.platys.name: wetty
      com.platys.description: A terminal window in Web-Browser
      com.platys.webui.title: WeTTY UI
      com.platys.webui.url: http://dataplatform:3001
    ports:
      - 3001:3000
    environment:
      - SSHHOST=${DOCKER_HOST_IP}
      - SSHPORT=22
      - SSHUSER=
      - SSHAUTH=password
      - PORT=3000
      - BASE=/
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: dannyben/madness:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.name: markdown-viewer
      com.platys.description: Platys Platform homepage viewer
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://dataplatform:80
    ports:
      - 80:3000
    volumes:
      - ./artefacts:/docs
      - ./conf/markdown-viewer/markdown-madness.yml:/docs/.madness.yml
      - ./data-transfer:/data-transfer
    command: server
    restart: unless-stopped
    healthcheck:
      test: [CMD-SHELL, curl -f http://markdown-viewer:3000 || exit 1]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 1m
  markdown-renderer:
    image: trivadis/jinja2-renderer:latest
    container_name: markdown-renderer
    hostname: markdown-renderer
    labels:
      com.platys.name: markdown-renderer
      com.platys.description: Platys Platform homepage rendering
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
      PLATYS_PLATFORM_NAME: bigdata-minio-platform
      PLATYS_PLATFORM_STACK: trivadis/platys-modern-data-platform
      PLATYS_PLATFORM_STACK_VERSION: develop
      PLATYS_COPY_COOKBOOK_DATA: 'True'
      SERVICE_LIST_VERSION: 2
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
    init: true
  #  ================================== data-provisioning ========================================== #
  data-provisioning:
    image: trivadis/platys-modern-data-platform-data:latest
    container_name: data-provisioning
    hostname: data-provisioning
    labels:
      com.platys.name: data-provisioning
      com.platys.description: Provisioning sample data
    volumes:
      - ./data-transfer:/data-transfer
    init: true
volumes:
  data-transfer-vol:
    name: data_transfer_vol
  aws-credentials-vol:
  nifi-config-vol:
  spark-3-5-3-vol:
