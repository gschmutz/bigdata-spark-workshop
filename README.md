# Big Data and Spark Workshop

Big Data Workshops with hands-on tutorials for working with S3, Spark, Delta Lake, Trino, ...

This workshop is part of the Trivadis course [Hadoop and Spark Ecosystem](https://www.trivadis.com/en/training/apache-hadoop-mapreduce-and-hadoop-ecosystem-bd-amh) as well as the [Big Data and Spark Ecosystem Module of the Data Engineering CAS](https://www.bfh.ch/ti/de/weiterbildung/cas/big-data/) at the Berner Fachhochschule.

All the workshops can be done on a container-based infrastructure using Docker Compose for the container orchestration. It can be run on a local machine or in a cloud environment. Check [01-environment](https://github.com/gschmutz/hadoop-workshop/tree/master/01-environment) for instructions on how to setup the infrastructure.
