# Big Data and Spark Workshop

Big Data Workshops with hands-on tutorials for working with S3, Spark, Delta Lake, Trino, ...

This workshop is used in the [Big Data and Spark Ecosystem Module of the Data Engineering CAS](https://www.bfh.ch/ti/de/weiterbildung/cas/big-data/) at the Berner Fachhochschule.

All the workshops can be done on a container-based infrastructure using Docker Compose for the container orchestration. It can be run on a local machine or in a cloud environment. Check [01-environment](https://github.com/gschmutz/hadoop-workshop/tree/master/01-environment) for instructions on how to setup the infrastructure.

## Workshops

  * [Working with Minio Object Storage](./02a-minio-object-storage)
  * [Working with AWS S3 Object Storage (optional)](./02b-aws-object-storage)
  * [Getting Started using Spark RDD and DataFrames](./03-spark-getting-started)
  * [Data Reading and Writing using DataFrames](./04-spark-dataframe)
  * [Graph Analysis using Spark GraphFrames](./05-spark-graphframe)
  * [Working with different data types](./06-data-types)
  * [Working with Delta Lake Table Format](./07-spark-deltalake)
  * [Working with Trino](./08-sql-on-bigdata)
  * [Data Ingestion with Apache NiFi](./09-data-ingestion-with-nifi)

